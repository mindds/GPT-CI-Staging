{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from math import ceil\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(default_credential, \"https://cognitiveservices.azure.com/.default\" )\n",
    "\n",
    "api_type = \"azure\"\n",
    "api_base = \"https://mgh-mind-data-science-private-e2-openai-service.openai.azure.com/\" \n",
    "api_version = \"2024-05-13-preview\"\n",
    "\n",
    "client = openai.AzureOpenAI(api_version=api_version, azure_endpoint=api_base, azure_ad_token_provider=token_provider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note = pd.read_csv('../../EDW Queries/Moura et al data-all-notes/Formatted_3year_moura_patient_notes_all.csv')\n",
    "df_dx = pd.read_csv('../../EDW Queries/Moura et al data-all-notes/patient_dx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note = df_note.merge(df_dx, on='PatientID', how='inner')\n",
    "df_note_grouped = df_note.groupby('PatientID')['NoteTXT'].apply(' '.join).reset_index()\n",
    "df_note_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_notes(notes, word_limit=32000):\n",
    "    \"\"\"\n",
    "    Splits patient notes into chunks without exceeding a word limit, ensuring that \n",
    "    a single note is not split across multiple chunks unless necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    notes (list of str): List of patient notes.\n",
    "    word_limit (int): Maximum number of words per chunk.\n",
    "    \n",
    "    Returns:\n",
    "    list of str: List of text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for note in notes:\n",
    "        note = \" \".join(note.split())\n",
    "        note_word_count = len(note.split())\n",
    "\n",
    "        if note_word_count > word_limit:\n",
    "            # If a single note is too long, split it at sentence level\n",
    "            sentences = re.split(r'(?<=[.!?]) +', note)  # Split into sentences\n",
    "            temp_chunk = []\n",
    "            temp_word_count = 0\n",
    "\n",
    "            for sentence in sentences:\n",
    "                sentence_word_count = len(sentence.split())\n",
    "\n",
    "                if temp_word_count + sentence_word_count > word_limit:\n",
    "                    chunks.append(\" \".join(temp_chunk).strip())\n",
    "                    temp_chunk = [sentence]  # Start a new chunk\n",
    "                    temp_word_count = sentence_word_count\n",
    "                else:\n",
    "                    temp_chunk.append(sentence)\n",
    "                    temp_word_count += sentence_word_count\n",
    "\n",
    "            if temp_chunk:\n",
    "                chunks.append(\" \".join(temp_chunk).strip())\n",
    "\n",
    "        else:\n",
    "            # If adding this note exceeds the limit, start a new chunk\n",
    "            if current_word_count + note_word_count > word_limit:\n",
    "                chunks.append(\" \".join(current_chunk).strip())  # Save previous chunk\n",
    "                current_chunk = [note]  # Start new chunk\n",
    "                current_word_count = note_word_count\n",
    "            else:\n",
    "                current_chunk.append(note)\n",
    "                current_word_count += note_word_count\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk).strip())\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_grouped['NoteChunk'] = df_note_grouped['NoteTXT'].apply(chunk_notes)\n",
    "df_note_grouped.to_csv(\"../../EDW Queries/Moura et al data-all-notes/grouped_patient_notes_dx.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_grouped = pd.read_csv(\"../../EDW Queries/Moura et al data-all-notes/grouped_patient_notes_dx.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_note_chunk(chunk_text):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"GPT-4o-model\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \n",
    "                        \"content\": \"You are a neurologist tasked with summarizing patient notes to assess cognitive impairment.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                        \"content\": f\"\"\"Here are the patient notes:\n",
    "                        {chunk_text}\n",
    "                    Please provide a concise summary focusing on the patient's cognitive health and any significant changes or diagnoses made during this period. \"\"\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize summaries step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with synthesizing summaries of patient notes to assess cognitive impairment over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. MCI \n",
    "                    3. Dementia\n",
    "                    Provide a rationale for your classification.\n",
    "                    Follow this format and do not include patient name in response: \n",
    "                    **syndromic diagnosis:**[Insert one of 3 categories above here]**\n",
    "                    **summarized reasons:**[Insert the summary of reasoning here]\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_syndromic_dx(response):\n",
    "    # Define the regex pattern to match the confidence level\n",
    "    pattern = r'\\*\\*syndromic diagnosis:\\*\\* (Normal|MCI|Dementia)'\n",
    "    # Search for the pattern in the response\n",
    "    match = re.search(pattern, response, re.IGNORECASE)\n",
    "    # Extract and return the confidence level if found\n",
    "    if match:\n",
    "        return match.group(1).lower()  # Convert to lowercase for consistency\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dx_to_category(value):\n",
    "    if value == 0:\n",
    "        return 'normal'\n",
    "    elif value == 2 or value == 3:\n",
    "        return 'mci'\n",
    "    elif value == 4:\n",
    "        return 'dementia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2 - Confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with synthesizing summaries of patient notes to assess cognitive impairment over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. MCI \n",
    "                    3. Dementia\n",
    "                    Provide a rationale for your classification. Additionally, provide a confidence score for the classification on a scale from 1 to 100, where 0 means completely uncertain and 100 means absolutely certain. Be very conservative with the scores.\n",
    "                    Follow this format and do not include patient name in response: \n",
    "                    **syndromic diagnosis:**[Insert one of 3 categories above here]**\n",
    "                    **confidence level:**[Insert confidence score 1-100 here]**\n",
    "                    **summarized reasons:**[Insert the summary of reasoning here]\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confidence_level(response):\n",
    "    match = re.search(r\"\\*\\*CDR Score:\\*\\*\\s*(\\d(\\.\\d)?)\", response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3 - separate summary and dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summaries(summaries):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with synthesizing summaries of patient notes to assess cognitive impairment over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summaries}\n",
    "                    Provide a summary of these summaries of the patient's cognitive health over the three-year period.\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx_of_summary(summary):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with making syndromic diagnoses of cognitive impairmen using summaries of patient notes over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here is the summary:\n",
    "                    {summary}\n",
    "                    Based on the summary, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. MCI \n",
    "                    3. Dementia\n",
    "                    Provide a rationale for your classification. Additionally, provide a confidence score for the classification on a scale from 1 to 100, where 0 means completely uncertain and 100 means absolutely certain. Be very conservative with the scores.\n",
    "                    Follow this format and do not include patient name in response: \n",
    "                    **syndromic diagnosis:**[Insert one of 3 categories above here]**\n",
    "                    **confidence level:**[Insert confidence score 1-100 here]**\n",
    "                    **justification:** [Insert a few sentences summarizing key observations and reasoning here]**\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 4 - all 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with synthesizing summaries of patient notes to assess cognitive impairment over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. Normal vs MCI (Mild Cognitive Impairment)\n",
    "                    3. MCI\n",
    "                    4. MCI vs Dementia\n",
    "                    5. Dementia\n",
    "                    Provide a rationale for your classification.\n",
    "                    Follow this format and do not include patient name in response: \n",
    "                    **Syndromic Diagnosis:**[Insert one of 5 categories above here]**\n",
    "                    **Justifications:**[Insert the summary of reasoning here]\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 5 - Logprob Revision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with making syndromic diagnoses of cognitive impairmen using summaries of patient notes over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. MCI \n",
    "                    3. Dementia\n",
    "                    Provide the response with just one number of one of the three categories above here, without any additional text or formatting.\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature = 0,\n",
    "                logprobs=True,\n",
    "                top_logprobs=2,\n",
    "                max_tokens=1\n",
    "            )\n",
    "            # Extract the classification (e.g., \"1\", \"2\", or \"3\")\n",
    "            predicted_class = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Extract top logprobs\n",
    "            top_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "            \n",
    "            # Format logprobs\n",
    "            formatted_logprobs = [\n",
    "                {\n",
    "                    \"token\": logprob.token,\n",
    "                    \"logprob\": logprob.logprob,\n",
    "                    \"probability_percent\": np.round(np.exp(logprob.logprob)*100,2)\n",
    "                }\n",
    "                for logprob in top_logprobs\n",
    "            ]\n",
    "            \n",
    "            return predicted_class, formatted_logprobs\n",
    "            # return response\n",
    "        \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_txt = \"../GPT/Results - sydronmic dx/Summaries of Notes/Z6354035_summaries.txt\"\n",
    "with open(summary_txt, 'r') as file:\n",
    "    patient_summary = file.read()\n",
    "output = summary_of_summary(patient_summary)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 6 - 5 Class Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with making syndromic diagnoses of cognitive impairmen using summaries of patient notes over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. Normal vs MCI\n",
    "                    3. MCI\n",
    "                    4. MCI vs Dementia\n",
    "                    5. Dementia\n",
    "                    Provide a rationale for your classification. Additionally, provide a confidence score for the classification on a scale from 1 to 100, where 0 means completely uncertain and 100 means absolutely certain. Be very conservative with the scores.\n",
    "                    Follow this format and do not include patient name in response: \n",
    "                    **syndromic diagnosis:**[Insert one of 5 categories above here]**\n",
    "                    **confidence level:**[Insert confidence score 1-100 here]**\n",
    "                    **justification:** [Insert a few sentences summarizing key observations and reasoning here]**\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature = 0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../GPT/Results - sydronmic dx/Summaries of Notes\"\n",
    "for patient_id in df_note_grouped['PatientID'].unique():\n",
    "    # Check if the summary file already exists\n",
    "    filename = os.path.join(input_dir, f'{patient_id}_summaries.txt')\n",
    "    if os.path.exists(filename):\n",
    "        # print(f\"File {filename} exists. Skipping patient {patient_id}...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"File {filename} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 7 - 5 Class Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with making syndromic diagnoses of cognitive impairmen using summaries of patient notes over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. Normal vs MCI\n",
    "                    3. MCI\n",
    "                    4. MCI vs Dementia\n",
    "                    5. Dementia\n",
    "                    Provide the response with just one number of one of the five categories above here, without any additional text or formatting.\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature = 0,\n",
    "                logprobs=True,\n",
    "                top_logprobs=2,\n",
    "                max_tokens=1\n",
    "            )\n",
    "            predicted_class = response.choices[0].message.content.strip()\n",
    "            \n",
    "            top_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "            \n",
    "            formatted_logprobs = [\n",
    "                {\n",
    "                    \"token\": logprob.token,\n",
    "                    \"logprob\": logprob.logprob,\n",
    "                    \"probability_percent\": np.round(np.exp(logprob.logprob)*100,2)\n",
    "                }\n",
    "                for logprob in top_logprobs\n",
    "            ]\n",
    "            \n",
    "            return predicted_class, formatted_logprobs\n",
    "            # return response\n",
    "        \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and run full experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = df_note_grouped['NoteChunk'].tolist()\n",
    "# notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_note_chunk(chunk_text):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \n",
    "                        \"content\": \"You are a neurologist tasked with summarizing patient notes to assess cognitive impairment.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                        \"content\": f\"\"\"Here are the patient notes:\n",
    "                        {chunk_text}\n",
    "                    Please provide a concise summary focusing on the patient's cognitive health and any significant changes or diagnoses made during this period.\"\"\"}\n",
    "                ]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_patient_save(patient_id, chunked_note, output_dir= \"../GPT/Results - sydronmic dx/Summaries of Notes\"):\n",
    "    single_patient_summary_list = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_summary = {executor.submit(summary_note_chunk, text): text for text in chunked_note}\n",
    "        \n",
    "        for future in as_completed(future_to_summary):\n",
    "            single_summary_output = future.result()\n",
    "            single_patient_summary_list.append(single_summary_output)\n",
    "    \n",
    "    # Join all the summaries for this patient\n",
    "    single_patient_joined_summary = \"\\n\\n\".join(single_patient_summary_list)\n",
    "    patient_summary_file = os.path.join(output_dir, f'{patient_id}_summaries.txt')\n",
    "    with open(patient_summary_file, 'w') as file:\n",
    "        file.write(single_patient_joined_summary)\n",
    "    \n",
    "    # Get the final diagnosis (summary of summaries)\n",
    "    dx_output = summary_of_summary(single_patient_joined_summary)\n",
    "    \n",
    "    return patient_id, dx_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_patients_in_batches(df, save_path, group_size=10, start_group_index=0):\n",
    "    total_patients = len(df)\n",
    "    num_groups = ceil(total_patients / group_size)\n",
    "    \n",
    "    for group_index in range(start_group_index, num_groups):\n",
    "        start_index = group_index * group_size\n",
    "        end_index = min((group_index + 1) * group_size, total_patients)\n",
    "\n",
    "        print(f\"Processing group {group_index + 1} of {num_groups}...\")\n",
    "        patient_summaries = {}\n",
    "        batch_df = df.iloc[start_index:end_index]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            future_to_patient = {executor.submit(process_single_patient_save, patient_id, chunked_note): patient_id \n",
    "                                 for patient_id, chunked_note in zip(batch_df['PatientID'], batch_df['NoteChunk'])}\n",
    "            \n",
    "            for future in as_completed(future_to_patient):\n",
    "                patient_id, dx_output = future.result()\n",
    "                patient_summaries[patient_id] = dx_output\n",
    "        \n",
    "        filename = f\"patient_group_{group_index + 1}_summaries.json\"\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "        # Save the results of this batch to a JSON file\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(patient_summaries, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Group {group_index + 1} processed and saved.\")\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../GPT/Results - sydronmic dx/GPT4o - Attempt 2\"\n",
    "# If starting from the beginning:\n",
    "process_multiple_patients_in_batches(df_note_grouped, save_path, group_size=20, start_group_index=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary_from_file(patient_id, input_dir='summaries'):\n",
    "    # Read the summary from the file\n",
    "    summary_file_path = os.path.join(input_dir, f'{patient_id}_summaries.txt')\n",
    "    with open(summary_file_path, 'r') as file:\n",
    "        patient_summary = file.read()\n",
    "    return patient_summary\n",
    "\n",
    "def process_single_patient(patient_id, input_dir):\n",
    "    # Read the patient's summary from the text file\n",
    "    summary_text = read_summary_from_file(patient_id, input_dir)\n",
    "    \n",
    "    # Generate the summary of summaries and diagnosis\n",
    "    final_summary = summary_of_summary(summary_text)\n",
    "    \n",
    "    return patient_id, final_summary\n",
    "\n",
    "def process_multiple_patients_in_batches(patient_ids, input_dir, output_dir, group_size=10, start_group_index=0):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    total_patients = len(patient_ids)\n",
    "    num_groups = ceil(total_patients / group_size)\n",
    "    \n",
    "    for group_index in range(start_group_index, start_group_index+1):\n",
    "        start_index = group_index * group_size\n",
    "        end_index = min((group_index + 1) * group_size, total_patients)\n",
    "        \n",
    "        patient_summaries = {}\n",
    "        batch_patient_ids = patient_ids[start_index:end_index]\n",
    "        print(batch_patient_ids)\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            future_to_patient = {executor.submit(process_single_patient, patient_id, input_dir): patient_id \n",
    "                                 for patient_id in batch_patient_ids}\n",
    "            \n",
    "            for future in as_completed(future_to_patient):\n",
    "                patient_id, result = future.result()\n",
    "                patient_summaries[patient_id] = result\n",
    "        \n",
    "        # Save the results of this batch to a JSON file\n",
    "        json_filename = os.path.join(output_dir, f'patient_group_{group_index + 1}_summaries.json')\n",
    "        with open(json_filename, 'w') as json_file:\n",
    "            json.dump(patient_summaries, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Group {group_index + 1} processed and saved to {json_filename}\")\n",
    "\n",
    "input_dir = \"../GPT/Results - sydronmic dx/Summaries of Notes\"\n",
    "output_dir = \"../GPT/Results - sydronmic dx/GPT4o - Attempt 2\"\n",
    "patient_ids = df_note_grouped['PatientID'].tolist()\n",
    "# Process all patients in batches and save results\n",
    "process_multiple_patients_in_batches(patient_ids, input_dir=input_dir, output_dir=output_dir, group_size=20, start_group_index=21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revision  - Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_summary(summary):\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-model2\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a neurologist tasked with making syndromic diagnoses of cognitive impairmen using summaries of patient notes over a three-year period.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": f\"\"\"Here are the summaries in chronological order:\n",
    "                    {summary}\n",
    "                    Based on these summaries, provide an overall classification of the patient's cognitive health over the three-year period into one of the following syndromic diagnoses:\n",
    "                    1. Normal\n",
    "                    2. MCI \n",
    "                    3. Dementia\n",
    "                    Provide the response with just one number of one of the three categories above here, without any additional text or formatting.\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature = 0.1,\n",
    "                logprobs=True,\n",
    "                top_logprobs=2,\n",
    "                max_tokens=1\n",
    "            )\n",
    "            # Extract the classification (e.g., \"1\", \"2\", or \"3\")\n",
    "            predicted_class = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Extract top logprobs\n",
    "            top_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n",
    "            \n",
    "            # Format logprobs\n",
    "            formatted_logprobs = [\n",
    "                {\n",
    "                    \"token\": logprob.token,\n",
    "                    \"logprob\": logprob.logprob,\n",
    "                    \"probability_percent\": np.round(np.exp(logprob.logprob)*100,2)\n",
    "                }\n",
    "                for logprob in top_logprobs\n",
    "            ]\n",
    "            \n",
    "            return {\n",
    "                \"prediction\": predicted_class,\n",
    "                \"logprobs\": formatted_logprobs\n",
    "            }\n",
    "            # return response\n",
    "        \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Rate limit exceeded. Retrying in 60 seconds...\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary_from_file(patient_id, input_dir='summaries'):\n",
    "    # Read the summary from the file\n",
    "    summary_file_path = os.path.join(input_dir, f'{patient_id}_summaries.txt')\n",
    "    with open(summary_file_path, 'r') as file:\n",
    "        patient_summary = file.read()\n",
    "    return patient_summary\n",
    "\n",
    "def process_single_patient(patient_id, input_dir):\n",
    "    # Read the patient's summary from the text file\n",
    "    summary_text = read_summary_from_file(patient_id, input_dir)\n",
    "    \n",
    "    # Generate the summary of summaries and diagnosis\n",
    "    final_summary = summary_of_summary(summary_text)\n",
    "    \n",
    "    return patient_id, final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_patients_in_batches(patient_ids, input_dir, output_dir, group_size=10, start_group_index=0):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    total_patients = len(patient_ids)\n",
    "    \n",
    "    num_groups = ceil(total_patients / group_size)\n",
    "    for group_index in range(start_group_index, num_groups):\n",
    "        start_index = group_index * group_size\n",
    "        end_index = min((group_index + 1) * group_size, total_patients)\n",
    "        print(f\"Processing group {group_index + 1} of {num_groups}...\")\n",
    "        \n",
    "        patient_summaries = {}\n",
    "        batch_patient_ids = patient_ids[start_index:end_index]\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            future_to_patient = {executor.submit(process_single_patient, patient_id, input_dir): patient_id \n",
    "                                for patient_id in batch_patient_ids}\n",
    "            \n",
    "            for future in as_completed(future_to_patient):\n",
    "                patient_id, result = future.result()\n",
    "                patient_summaries[patient_id] = result\n",
    "        \n",
    "        # Save the results of this batch to a JSON file\n",
    "        json_filename = os.path.join(output_dir, f'patient_group_{group_index + 1}_logprobs.json')\n",
    "        with open(json_filename, 'w') as json_file:\n",
    "            json.dump(patient_summaries, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Group {group_index + 1} processed and saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../GPT/Results - sydronmic dx/Summaries of Notes\"\n",
    "output_dir = \"../GPT/Results - sydronmic dx/GPT4o - Attempt 5 - Revision\"\n",
    "\n",
    "patient_ids = df_note_grouped['PatientID'].tolist()\n",
    "# Process all patients in batches and save results\n",
    "process_multiple_patients_in_batches(patient_ids, input_dir=input_dir, output_dir=output_dir, group_size=20, start_group_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../GPT/Results - sydronmic dx/Summaries of Notes\"\n",
    "output_dir = \"../GPT/Results - sydronmic dx/GPT4o - Attempt 8 - Revision\"\n",
    "patient_ids = df_note_grouped['PatientID'].tolist()\n",
    "# Process all patients in batches and save results\n",
    "process_multiple_patients_in_batches(patient_ids, input_dir=input_dir, output_dir=output_dir, group_size=20, start_group_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Class logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../GPT/Results - sydronmic dx/Summaries of Notes\"\n",
    "output_dir = \"../GPT/Results - sydronmic dx/GPT4o - Attempt 9 - Revision\"\n",
    "\n",
    "patient_ids = df_note_grouped['PatientID'].tolist()\n",
    "# Process all patients in batches and save results\n",
    "process_multiple_patients_in_batches(patient_ids, input_dir=input_dir, output_dir=output_dir, group_size=20, start_group_index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
