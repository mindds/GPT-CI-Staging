{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\" # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\" # export OPENBLAS_NUM_THREADS=4 \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"6\" # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\" # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"6\" # export NUMEXPR_NUM_THREADS=6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import ast\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from scipy.stats import rankdata, spearmanr, shapiro, ttest_rel, wilcoxon\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "folder_name_main = 'results-predictions-RS'\n",
    "ids_ls = []\n",
    "df_ls = []\n",
    "for m in range(n):\n",
    "    file_name = f'{folder_name_main}/fold_{m}_predictions.csv'\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['fold'] = m\n",
    "    ids = df['PatientID'].tolist()\n",
    "    ids_ls.append(ids)\n",
    "    df_ls.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dx_to_category(value):\n",
    "    if value == 0:\n",
    "        return 'Normal'\n",
    "    elif value == 1:\n",
    "        return 'MCI'\n",
    "    elif value == 2:\n",
    "        return 'Dementia'\n",
    "\n",
    "df_ls[0]['actual_category'] = df_ls[0]['syndromic_dx'].apply(convert_dx_to_category)\n",
    "\n",
    "df_ls[0]['predicted_category'] = df_ls[0]['predicted_dx'].apply(convert_dx_to_category)\n",
    "df_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['Normal','MCI','Dementia']\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(df_ls[0]['actual_category'], df_ls[0]['predicted_category'], labels=all_labels)\n",
    "cm\n",
    "\n",
    "\n",
    "# # Convert the confusion matrix into a DataFrame for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index=all_labels, columns=all_labels)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"YlGnBu\", fmt=\"d\", annot_kws={\"size\": 24})\n",
    "plt.title('Heatmap of Actual vs. Predicted CI Stage', fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=20)  \n",
    "plt.ylabel('Actual', fontsize=20)  \n",
    "plt.xticks(fontsize=16)  \n",
    "plt.yticks(fontsize=16)  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT and Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baccianella_mse(y_true, y_pred, num_classes):\n",
    "    n = len(y_true)\n",
    "    mse = 0\n",
    "    for i in range(n):\n",
    "        mse += ((y_true[i] - y_pred[i]) / (num_classes - 1)) ** 2\n",
    "    return mse / n\n",
    "\n",
    "def plot_confusion_matrices(xgb_conf_matrix, gpt_conf_matrix, fold):\n",
    "    \"\"\"\n",
    "    Plots and saves confusion matrices for XGBoost and GPT side by side in subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    - xgb_conf_matrix: XGBoost confusion matrix\n",
    "    - gpt_conf_matrix: GPT confusion matrix\n",
    "    - fold: Fold number (for the title and filename)\n",
    "    - save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    labels = ['Normal', 'MCI', 'Dementia'] \n",
    "\n",
    "    # Subplot 1: XGBoost Confusion Matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(xgb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'USE + XGBoost Fold {fold} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "    # Subplot 2: GPT Confusion Matrix (if available)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(gpt_conf_matrix, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'GPT Fold {fold} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "def compare_models_with_gpt(xgb_predictions_path, gpt_predictions_df, n_folds):\n",
    "    \"\"\"\n",
    "    Compares XGBoost and GPT predictions for each fold.\n",
    "    Calculates accuracy, quadratic Cohen's kappa, and confusion matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - xgb_predictions_path: Path where XGBoost fold predictions are saved (as CSV files)\n",
    "    - gpt_predictions_df: DataFrame containing GPT predictions, indexed by PatientID\n",
    "    - n_folds: Number of cross-validation folds\n",
    "    - save_path: Path to save the results (metrics and confusion matrices)\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing metrics (accuracy, kappa) for both XGBoost and GPT for each fold\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Load XGBoost predictions for this fold\n",
    "        xgb_pred_path = f'{xgb_predictions_path}fold_{fold}_predictions.csv'\n",
    "        xgb_df = pd.read_csv(xgb_pred_path)\n",
    "        # Get the corresponding GPT predictions for this fold using PatientID\n",
    "        fold_gpt_predictions = gpt_predictions_df[gpt_predictions_df['PatientID'].isin(xgb_df['PatientID'])]\n",
    "\n",
    "        # Align both dataframes on PatientID\n",
    "        merged_df = pd.merge(xgb_df, fold_gpt_predictions, on='PatientID', how='inner')\n",
    "\n",
    "        # If GPT predictions are missing, fill them with NaN or a default value\n",
    "        merged_df['predicted_cat'] = merged_df['predicted_cat'].fillna(-1)  # Assign -1 for missing GPT predictions\n",
    "        # Extract true labels and predictions\n",
    "        true_labels = merged_df['syndromic_dx'].values\n",
    "        xgb_pred = merged_df['predicted_dx'].values\n",
    "        gpt_pred = merged_df['predicted_cat'].values\n",
    "\n",
    "        # Exclude cases where GPT could not generate a prediction (gpt_pred == -1)\n",
    "        valid_indices = gpt_pred != -1\n",
    "        true_labels_gpt = true_labels[valid_indices]\n",
    "        gpt_pred_valid = gpt_pred[valid_indices]\n",
    "\n",
    "        # Calculate metrics for XGBoost\n",
    "        xgb_acc = accuracy_score(true_labels, xgb_pred)\n",
    "        xgb_kappa = cohen_kappa_score(true_labels, xgb_pred, weights='quadratic')\n",
    "        xgb_spearman, _ = spearmanr(true_labels, xgb_pred)\n",
    "        xgb_baccianella_mse = baccianella_mse(true_labels, xgb_pred, 3)\n",
    "        # print(xgb_spearman)\n",
    "        # Calculate metrics for GPT (only for cases where GPT generated predictions)\n",
    "        if len(true_labels_gpt) > 0:\n",
    "            gpt_acc = accuracy_score(true_labels_gpt, gpt_pred_valid)\n",
    "            gpt_kappa = cohen_kappa_score(true_labels_gpt, gpt_pred_valid, weights='quadratic')\n",
    "            gpt_spearman, _ = spearmanr(true_labels_gpt, gpt_pred_valid)\n",
    "            gpt_baccianella_mse = baccianella_mse(true_labels_gpt, gpt_pred_valid, 3)\n",
    "        else:\n",
    "            gpt_acc, gpt_kappa = np.nan, np.nan  # Handle cases where no GPT predictions are available\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Fold': fold,\n",
    "            'XGB_Accuracy': xgb_acc,\n",
    "            'XGB_Kappa': xgb_kappa,\n",
    "            'XGB_Spearman': xgb_spearman,\n",
    "            'XGB_MSE': xgb_baccianella_mse,\n",
    "            'GPT_Accuracy': gpt_acc,\n",
    "            'GPT_Kappa': gpt_kappa,\n",
    "            'GPT_Spearman': gpt_spearman,\n",
    "            'GPT_MSE': gpt_baccianella_mse,\n",
    "        })\n",
    "\n",
    "        # Confusion matrix for XGBoost\n",
    "        xgb_conf_matrix = confusion_matrix(true_labels, xgb_pred)\n",
    "        # plot_confusion_matrix(xgb_conf_matrix, f'XGBoost Fold {fold}')\n",
    "\n",
    "        # # Confusion matrix for GPT\n",
    "        gpt_conf_matrix = confusion_matrix(true_labels, gpt_pred)\n",
    "        # plot_confusion_matrix(gpt_conf_matrix, f'GPT Fold {fold}')\n",
    "        plot_confusion_matrices(xgb_conf_matrix, gpt_conf_matrix, fold)\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    # results_df.to_csv(f'{save_path}comparison_results.csv', index=False)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_predictions_df = pd.read_csv(\"../Results - sydronmic dx/Files/gpt_ci_attempt4_actual_conf.csv\")\n",
    "df_certainty = pd.read_csv('../../../../../Prelim_Data_R/Dementia_ReferenceStandardDataset_08292019_subset_mrn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_num(value):\n",
    "    if value == 'Normal':\n",
    "        return 0\n",
    "    elif value == 'MCI':\n",
    "        return 1\n",
    "    elif value == 'Dementia':\n",
    "        return 2\n",
    "    \n",
    "gpt_predictions_df['predicted_cat'] = gpt_predictions_df['predicted_category'].apply(convert_str_to_num)\n",
    "gpt_predictions_df = gpt_predictions_df[['PatientID', 'predicted_cat', 'ResponseTXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compare_models_with_gpt('./results-predictions-RS/', gpt_predictions_df, n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DementiaBERT and Hybrid Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baccianella_mse(y_true, y_pred, num_classes):\n",
    "    n = len(y_true)\n",
    "    mse = 0\n",
    "    for i in range(n):\n",
    "        mse += ((y_true[i] - y_pred[i]) / (num_classes - 1)) ** 2\n",
    "    return mse / n\n",
    "\n",
    "def dementiabert_metrics(dbert_predictions_path, n_folds, model_name):\n",
    "    \"\"\"\n",
    "    Compares dbertoost and GPT predictions for each fold.\n",
    "    Calculates accuracy, quadratic Cohen's kappa, and confusion matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - dbert_predictions_path: Path where dbertoost fold predictions are saved (as CSV files)\n",
    "    - gpt_predictions_df: DataFrame containing GPT predictions, indexed by PatientID\n",
    "    - n_folds: Number of cross-validation folds\n",
    "    - save_path: Path to save the results (metrics and confusion matrices)\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing metrics (accuracy, kappa) for both dbertoost and GPT for each fold\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Load dbertoost predictions for this fold\n",
    "        dbert_pred_path = f'{dbert_predictions_path}fold_{fold}_predictions.csv'\n",
    "        dbert_df = pd.read_csv(dbert_pred_path)\n",
    "        true_labels = dbert_df['syndromic_dx'].values\n",
    "        dbert_pred = dbert_df['predicted_dx'].values\n",
    "        # compute metrics\n",
    "        # kappa\n",
    "        #\n",
    "        # Calculate metrics for dbertoost\n",
    "        # dbert_acc = accuracy_score(true_labels, dbert_pred)\n",
    "        dbert_kappa = cohen_kappa_score(true_labels, dbert_pred, weights='quadratic')\n",
    "        dbert_spearman, _ = spearmanr(true_labels, dbert_pred)\n",
    "        dbert_baccianella_mse = baccianella_mse(true_labels, dbert_pred, 3)\n",
    "\n",
    "        results.append({\n",
    "            'Fold': fold,\n",
    "            # f'{model_name}_Accuracy': dbert_acc,\n",
    "            f'{model_name}_Kappa': dbert_kappa,\n",
    "            f'{model_name}_Spearman': dbert_spearman,\n",
    "            f'{model_name}_MSE': dbert_baccianella_mse,\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_rs_path = \"../Results - sydronmic dx/Files/results-predictions-dementiabert-RS/\"\n",
    "dbert_summaries_path = \"../Results - sydronmic dx/Files/results-predictions-dementiabert-summaries/\"\n",
    "xgb2_df = dementiabert_metrics(dbert_rs_path, 10, 'XGB2')\n",
    "xgb3_df = dementiabert_metrics(dbert_summaries_path, 10, 'XGB3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_multimetrics = results_df.merge(xgb2_df, on='Fold', how='inner')\n",
    "results_df_multimetrics = results_df_multimetrics.merge(xgb3_df, on='Fold', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_kappas = results_df_multimetrics.XGB_Kappa.mean()\n",
    "std_kappas = results_df_multimetrics.XGB_Kappa.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Kappa Scores:\")\n",
    "print(mean_kappas)\n",
    "\n",
    "print(\"\\nStandard Deviation of Kappa Scores:\")\n",
    "print(std_kappas)\n",
    "\n",
    "scores = results_df_multimetrics['XGB_Kappa'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_kappas = results_df_multimetrics.XGB2_Kappa.mean()\n",
    "std_kappas = results_df_multimetrics.XGB2_Kappa.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Kappa Scores:\")\n",
    "print(mean_kappas)\n",
    "\n",
    "print(\"\\nStandard Deviation of Kappa Scores:\")\n",
    "print(std_kappas)\n",
    "\n",
    "scores = results_df_multimetrics['XGB2_Kappa'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_kappas = results_df_multimetrics.XGB3_Kappa.mean()\n",
    "std_kappas = results_df_multimetrics.XGB3_Kappa.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Kappa Scores:\")\n",
    "print(mean_kappas)\n",
    "\n",
    "print(\"\\nStandard Deviation of Kappa Scores:\")\n",
    "print(std_kappas)\n",
    "\n",
    "scores = results_df_multimetrics['XGB3_Kappa'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_kappas = results_df_multimetrics.GPT_Kappa.mean()\n",
    "std_kappas = results_df_multimetrics.GPT_Kappa.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean Kappa Scores:\")\n",
    "print(mean_kappas)\n",
    "\n",
    "print(\"\\nStandard Deviation of Kappa Scores:\")\n",
    "print(std_kappas)\n",
    "\n",
    "scores = results_df_multimetrics['GPT_Kappa'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_multimetrics = pd.read_csv(\"../Results - sydronmic dx/Files/df_all_models_multimetrics_comparison.csv\")\n",
    "results_df_multimetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "# Bar width\n",
    "bar_width = 0.23\n",
    "index = np.arange(10)\n",
    "\n",
    "# Use the YlGnBu color palette\n",
    "colors = sns.color_palette(\"YlGnBu\", 4)\n",
    "\n",
    "# Create subplots\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "fig, ax = plt.subplots(figsize=(24, 8))  # Example size: 10 inches wide, 6 inches tall\n",
    "bars_xgboost = ax.bar(index, results_df_multimetrics['XGB_Kappa'].tolist(), bar_width, label='USE Framework', color=colors[0])\n",
    "bars_xgb_model2 = ax.bar(index + bar_width, results_df_multimetrics['XGB2_Kappa'].tolist(), bar_width, label='DementiaBERT Framework', color=colors[1])\n",
    "bars_xgb_model3 = ax.bar(index + 2 * bar_width, results_df_multimetrics['XGB3_Kappa'].tolist(), bar_width, label='Hybrid Framework', color=colors[2])\n",
    "bars_gpt = ax.bar(index + 3 * bar_width, results_df_multimetrics['GPT_Kappa'].tolist(), bar_width, label='GPT-4o-Powered Framework', color=colors[3])\n",
    "\n",
    "# Add text labels for each bar (USE)\n",
    "for bar in bars_xgboost:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2 - 0.06, height, f'{height:.2f}', ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "\n",
    "# Add text labels for each bar (DementiaBERT)\n",
    "for bar in bars_xgb_model2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2 - 0.02, height, f'{height:.2f}', ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "# Add text labels for each bar (Hybrid)\n",
    "for bar in bars_xgb_model3:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2 + 0.02, height, f'{height:.2f}', ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "\n",
    "# Add text labels for each bar (GPT)\n",
    "for bar in bars_gpt:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2 + 0.06, height, f'{height:.2f}', ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylim(0.4, 1)  # Set the range from 0.4 to 1\n",
    "ax.set_xlabel('Folds', fontsize=28)\n",
    "ax.set_ylabel(\"Weighted Cohen's Kappa\", fontsize=28)\n",
    "ax.set_xticks(index + 1.5 * bar_width)\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(10)], fontsize=26)\n",
    "ax.tick_params(axis='y', labelsize=26)\n",
    "\n",
    "ax.set_xlim([-0.3, 10])\n",
    "ax.legend(loc='lower right', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_spearman = results_df_multimetrics.XGB_Spearman.mean()\n",
    "std_spearman = results_df_multimetrics.XGB_Spearman.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean spearman Scores:\")\n",
    "print(mean_spearman)\n",
    "\n",
    "print(\"\\nStandard Deviation of spearman Scores:\")\n",
    "print(std_spearman)\n",
    "\n",
    "# Example scores from 10 folds\n",
    "scores = results_df_multimetrics['XGB_Spearman'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_spearman = results_df_multimetrics.XGB2_Spearman.mean()\n",
    "std_spearman = results_df_multimetrics.XGB2_Spearman.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean spearman Scores:\")\n",
    "print(mean_spearman)\n",
    "\n",
    "print(\"\\nStandard Deviation of spearman Scores:\")\n",
    "print(std_spearman)\n",
    "\n",
    "# Example scores from 10 folds\n",
    "scores = results_df_multimetrics['XGB2_Spearman'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_spearman = results_df_multimetrics.XGB3_Spearman.mean()\n",
    "std_spearman = results_df_multimetrics.XGB3_Spearman.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean spearman Scores:\")\n",
    "print(mean_spearman)\n",
    "\n",
    "print(\"\\nStandard Deviation of spearman Scores:\")\n",
    "print(std_spearman)\n",
    "\n",
    "scores = results_df_multimetrics['XGB3_Spearman'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each model\n",
    "mean_spearman = results_df.GPT_Spearman.mean()\n",
    "std_spearman = results_df.GPT_Spearman.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean spearman Scores:\")\n",
    "print(mean_spearman)\n",
    "\n",
    "print(\"\\nStandard Deviation of spearman Scores:\")\n",
    "print(std_spearman)\n",
    "\n",
    "scores = results_df_multimetrics['GPT_Spearman'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = results_df_multimetrics.XGB_MSE.mean()\n",
    "std_mse = results_df_multimetrics.XGB_MSE.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean mse Scores:\")\n",
    "print(mean_mse)\n",
    "\n",
    "print(\"\\nStandard Deviation of mse Scores:\")\n",
    "print(std_mse)\n",
    "\n",
    "scores = results_df_multimetrics['XGB_MSE'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = results_df_multimetrics.XGB2_MSE.mean()\n",
    "std_mse = results_df_multimetrics.XGB2_MSE.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean mse Scores:\")\n",
    "print(mean_mse)\n",
    "\n",
    "print(\"\\nStandard Deviation of mse Scores:\")\n",
    "print(std_mse)\n",
    "\n",
    "scores = results_df_multimetrics['XGB2_MSE'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = results_df_multimetrics.XGB3_MSE.mean()\n",
    "std_mse = results_df_multimetrics.XGB3_MSE.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean mse Scores:\")\n",
    "print(mean_mse)\n",
    "\n",
    "print(\"\\nStandard Deviation of mse Scores:\")\n",
    "print(std_mse)\n",
    "\n",
    "scores = results_df_multimetrics['XGB3_MSE'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mse = results_df.GPT_MSE.mean()\n",
    "std_mse = results_df.GPT_MSE.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean mse Scores:\")\n",
    "print(mean_mse)\n",
    "\n",
    "print(\"\\nStandard Deviation of mse Scores:\")\n",
    "print(std_mse)\n",
    "\n",
    "scores = results_df_multimetrics['GPT_MSE'].tolist()\n",
    "# Calculate mean and SD\n",
    "mean_score = np.mean(scores)\n",
    "sd_score = np.std(scores, ddof=1)  # ddof=1 for sample standard deviation\n",
    "\n",
    "n = len(scores)\n",
    "t_value = stats.t.ppf(0.975, n - 1)\n",
    "margin_of_error = t_value * (sd_score / np.sqrt(n))\n",
    "\n",
    "ci_lower = mean_score - margin_of_error\n",
    "ci_upper = mean_score + margin_of_error\n",
    "\n",
    "print(f\"\\n95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgroup Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = pd.read_csv(\"../../../EDW Utilities/folder for JH-patient-features/jh_patient_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sex_subgroup(xgb_predictions_path, gpt_predictions_df, n_folds):\n",
    "    \"\"\"\n",
    "    Compares XGBoost and GPT predictions for each fold.\n",
    "    Calculates accuracy, quadratic Cohen's kappa, and confusion matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - xgb_predictions_path: Path where XGBoost fold predictions are saved (as CSV files)\n",
    "    - gpt_predictions_df: DataFrame containing GPT predictions, indexed by PatientID\n",
    "    - n_folds: Number of cross-validation folds\n",
    "    - save_path: Path to save the results (metrics and confusion matrices)\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing metrics (accuracy, kappa) for both XGBoost and GPT for each fold\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Load XGBoost predictions for this fold\n",
    "        xgb_pred_path = f'{xgb_predictions_path}fold_{fold}_predictions.csv'\n",
    "        xgb_df = pd.read_csv(xgb_pred_path)\n",
    "        # Get the corresponding GPT predictions for this fold using PatientID\n",
    "        fold_gpt_predictions = gpt_predictions_df[gpt_predictions_df['PatientID'].isin(xgb_df['PatientID'])]\n",
    "\n",
    "        # Align both dataframes on PatientID\n",
    "        merged_df = pd.merge(xgb_df, fold_gpt_predictions, on='PatientID', how='inner')\n",
    "        demo_df = pd.merge(merged_df, df_demo, how='inner', on='PatientID')\n",
    "        \n",
    "        # dividing\n",
    "        male_df = demo_df[demo_df['SexDSC']=='Male']\n",
    "        female_df = demo_df[demo_df['SexDSC']=='Female']\n",
    "\n",
    "        # Calculate metrics for XGBoost\n",
    "        xgb_male = cohen_kappa_score(male_df['syndromic_dx'], male_df['predicted_dx'], weights='quadratic')\n",
    "        xgb_female = cohen_kappa_score(female_df['syndromic_dx'], female_df['predicted_dx'], weights='quadratic')\n",
    "\n",
    "        gpt_male = cohen_kappa_score(male_df['syndromic_dx'], male_df['predicted_cat'], weights='quadratic')\n",
    "        gpt_female = cohen_kappa_score(female_df['syndromic_dx'], female_df['predicted_cat'], weights='quadratic')\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'Fold': fold,\n",
    "            'XGB_male': xgb_male,\n",
    "            'XGB_female': xgb_female,\n",
    "            'GPT_male': gpt_male,\n",
    "            'GPT_female': gpt_female,\n",
    "        })\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = sex_subgroup('results-predictions-RS/', gpt_predictions_df, n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dementiabert_sex_subgroup(dbert_predictions_path, n_folds, model_name):\n",
    "    \"\"\"\n",
    "    Compares dbertoost and GPT predictions for each fold.\n",
    "    Calculates accuracy, quadratic Cohen's kappa, and confusion matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    - dbert_predictions_path: Path where dbertoost fold predictions are saved (as CSV files)\n",
    "    - gpt_predictions_df: DataFrame containing GPT predictions, indexed by PatientID\n",
    "    - n_folds: Number of cross-validation folds\n",
    "    - save_path: Path to save the results (metrics and confusion matrices)\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing metrics (accuracy, kappa) for both dbertoost and GPT for each fold\n",
    "    \"\"\"\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Load dbertoost predictions for this fold\n",
    "        dbert_pred_path = f'{dbert_predictions_path}fold_{fold}_predictions.csv'\n",
    "        dbert_df = pd.read_csv(dbert_pred_path)\n",
    "        demo_df = pd.merge(dbert_df, df_demo, how='inner', on='PatientID')\n",
    "        \n",
    "        male_df = demo_df[demo_df['SexDSC']=='Male']\n",
    "        female_df = demo_df[demo_df['SexDSC']=='Female']\n",
    "\n",
    "        dbert_male = cohen_kappa_score(male_df['syndromic_dx'], male_df['predicted_dx'], weights='quadratic')\n",
    "        dbert_female = cohen_kappa_score(female_df['syndromic_dx'], female_df['predicted_dx'], weights='quadratic')\n",
    "\n",
    "        results.append({\n",
    "            'Fold': fold,\n",
    "            # f'{model_name}_Accuracy': dbert_acc,\n",
    "            f'{model_name}_male': dbert_male,\n",
    "            f'{model_name}_female': dbert_female,\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2_sex_df = dementiabert_sex_subgroup(dbert_rs_path, 10, 'XGB2')\n",
    "xgb3_sex_df = dementiabert_sex_subgroup(dbert_summaries_path, 10, 'XGB3')\n",
    "\n",
    "merged_sex_df = sex_df.merge(xgb2_sex_df, on='Fold', how='inner')\n",
    "merged_sex_df = merged_sex_df.merge(xgb3_sex_df, on='Fold', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = merged_sex_df.melt(id_vars=['Fold'], \n",
    "                      var_name='Model_Sex', \n",
    "                      value_name='Kappa')\n",
    "# Split the 'Model_Sex' column into separate 'Model' and 'Sex' columns\n",
    "df_long[['Model', 'Sex']] = df_long['Model_Sex'].str.split('_', expand=True)\n",
    "\n",
    "# Example dictionary mapping\n",
    "mapping_dict = {'XGB': 'USE', 'XGB2': 'DementiaBERT','XGB3': 'Hybrid', 'GPT': 'GPT-4o-Powered'}\n",
    "df_long['Model'] = df_long['Model'].map(mapping_dict)\n",
    "mapping_dict_sex = {'male': 'Male', 'female': 'Female'}\n",
    "df_long['Sex'] = df_long['Sex'].map(mapping_dict_sex)\n",
    "\n",
    "custom_order = ['USE', 'DementiaBERT', 'Hybrid', 'GPT-4o-Powered']\n",
    "\n",
    "# Set up the figure\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "plt.figure(figsize=(14, 9.5))\n",
    "\n",
    "palette = sns.color_palette(\"YlGnBu\", 2)  # Assuming you have two hues for Male and Female\n",
    "\n",
    "# Define custom flier properties for each hue (male and female)\n",
    "flierprops_male = {\n",
    "    \"marker\": \"o\",  # Circle marker for outliers\n",
    "    \"markersize\": 8,  # Size of the marker\n",
    "    \"markerfacecolor\": palette[0],  # Match the Male box color\n",
    "    \"markeredgecolor\": \"none\",  # Remove edge color\n",
    "}\n",
    "\n",
    "flierprops_female = {\n",
    "    \"marker\": \"o\",\n",
    "    \"markersize\": 8,\n",
    "    \"markerfacecolor\": palette[1],  # Match the Female box color\n",
    "    \"markeredgecolor\": \"none\",\n",
    "}\n",
    "\n",
    "# Create boxplot with YlGnBu palette\n",
    "ax = sns.boxplot(x='Model', y='Kappa', hue='Sex', data=df_long, order=custom_order, palette=palette, width=0.6)\n",
    "models = df_long['Model'].unique()\n",
    "ax.set_ylim(0.4, 1.05)  # Set slightly above 1 to give extra space\n",
    "\n",
    "for i, line in enumerate(ax.lines):\n",
    "    if i % 6 == 5:  # Outliers\n",
    "        x_data = line.get_xdata()  # Get x-coordinates of outliers\n",
    "        y_data = line.get_ydata()  # Get y-coordinates of outliers\n",
    "        print(x_data)\n",
    "        for x, y in zip(x_data, y_data):\n",
    "            # Check if the outlier belongs to Male or Female based on its x-coordinate\n",
    "            if \".85\" in str(x):  # Replace with your logic to differentiate\n",
    "                color = palette[0]  # Male color\n",
    "            else:\n",
    "                color = palette[1]  # Female color\n",
    "            \n",
    "            # Scatter the point with the appropriate color\n",
    "            ax.scatter(x, y, color=color, zorder=3, edgecolor='black', linewidth=0.5, s=150)\n",
    "\n",
    "plt.xlabel(\"Frameworks\", fontsize=35)\n",
    "plt.ylabel(\"Weighted Cohen's Kappa\", fontsize=35)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=32)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=32)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
